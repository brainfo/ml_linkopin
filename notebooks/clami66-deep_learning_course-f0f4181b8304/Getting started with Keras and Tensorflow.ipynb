{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Keras and Tensorflow\n",
    "\n",
    "<img src=\"figures/keras-tensorflow-logo.jpg\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "* Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "* Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "* Runs seamlessly on CPU and GPU.\n",
    "\n",
    "We will be mostly writing python code using Keras libraries, but \"under the hood\" Keras is using tensorflow libraries.\n",
    "\n",
    "The documentation is at [keras.io](https://keras.io).\n",
    "\n",
    "## If you want to use R instead of python\n",
    "\n",
    "Keras can run on R as well, it will look quite similar to the code in this notebook (but, you know, with <- instead of =). You find the docs here: https://keras.rstudio.com/\n",
    "\n",
    "## Installing the relevant libraries\n",
    "\n",
    "\n",
    "\n",
    "Setting up Keras and Tensorflow is pretty simple if you have Anaconda installed.\n",
    "\n",
    "You can either install the environment in the git repository:\n",
    "\n",
    "```\n",
    "$ conda env create -f environment.yaml\n",
    "```\n",
    "\n",
    "And then activate it before launching jupyter:\n",
    "\n",
    "```\n",
    "$ conda activate medbioinfo_deep_learning\n",
    "$ jupyter-notebook\n",
    "```\n",
    "\n",
    "These commands can be run from a shell on Linux or Mac. If you have Windows, it might be easier to use Anaconda Navigator.\n",
    "\n",
    "If you don't want to create a new conda environment for some reason, you can launch the following commands from your shell or even from this notebook. These packages will then be installed on your base environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install numpy==1.19.5\n",
    "!conda install tensorflow>=2.0.0\n",
    "!conda install keras>=2.4.3\n",
    "!conda install matplotlib\n",
    "!conda install mpld3\n",
    "!conda install pydot\n",
    "!conda install graphviz\n",
    "!conda install scikit-learn\n",
    "!conda install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a tensor\n",
    "\n",
    "The main variables in TensorFlow are, of course, tensors:\n",
    "\n",
    "> A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually  such a tensor), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize. The dimension of the tensor is called its rank.\n",
    "\n",
    "> TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors.\n",
    "\n",
    "So, in this case, a neural network is defined in TensorFlow as graphs through which the data flows until the final result is produced.\n",
    "\n",
    "## The first step is to build a graph of operations\n",
    "\n",
    "* NNs are defined in TensorFlow as graphs through which the data flows until the final result is produced\n",
    "* Before we can do any operation on our data (images, etc) we need to build the graph of tensor operations\n",
    "* When we have a full graph built from input to output, we can run (flow) our data (training or testing) through it.\n",
    "\n",
    "\n",
    "## Tensors and data are *not* the same thing\n",
    "* Tensors are, rather, a symbolic representation of the data\n",
    "* Think about the function $g = f(x)$: as long as we do not assign a value to x, we will not have a fully computed g\n",
    "* In this case, $g$ is the output tensor, $x$ the input tensor, $f$ the tensor operation\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "* We have a set of color images of size 1000x1000 pixels (1 megapixel) that we want to use on our NN \n",
    "* We define tensors with shape $(n, 1000, 1000, 3)$\n",
    "    * $n$ is the number of images that we are presenting to our network in one go (\"batch block\")\n",
    "    * 1000\\*1000: image pixels\n",
    "    * 3 is the number of channels (RGB)\n",
    "    * Grayscale images tensors would have shape $(n, 1000, 1000, 1)$\n",
    "\n",
    "## One thing to remember when operating on tensors\n",
    "\n",
    "The dimensions between tensors coming out of the n-th node and those going into the (n+1)-th node *must* match:\n",
    "\n",
    "* If each sample in our dataset is made of 10 features, the first (input) layer must accept a tensor of shape (n, 10)\n",
    "* If the first layer in our NN outputs a 3D tensor, the second layer must accept a 3D tensor as input\n",
    "* Check the documentation to make sure what input-output shapes are allowed\n",
    "\n",
    "## Here's how a NN layer looks like in TensorFlow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/run_metadata_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer (\"layer1) has:\n",
    "\n",
    "* 764 inputs\n",
    "* 500 neurons\n",
    "* How many weights, then?\n",
    "* How many biases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is how a model is built and trained in Keras\n",
    "\n",
    "Everything fits in five lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Multi-layer perceptron (one hidden layer)\n",
    "#Logistic activations, one output\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=3, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Gradient descent algorithm, Mean Squared Error as Loss function\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "\n",
    "#Training for 10 iterations of the data (epochs)\n",
    "history = model.fit(data, labels, epochs=10, batch=data.shape[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does each bit do? And how do we change the code to solve different problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network in Keras is called a Model\n",
    "\n",
    "The simplest kind of model is of the Sequential kind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an \"empty\" model, with no layers, no inputs or outputs are defined either.\n",
    "\n",
    "Adding layer is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=3, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A \"Dense\" layer is a fully connected layer as the ones we have seen in Multi-layer Perceptrons.\n",
    "The above is equal to having this network:\n",
    "\n",
    "<img src=\"figures/simplenet.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see the layers in the Model this far, we can just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Notice the number of parameters, can you tell why 12 and 8 parameters for the first and second layers respectively?\n",
    "\n",
    "## Other ways of visualizing a model\n",
    "If you want a nice figure for your next paper, you could try using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, \"figures/simplenet_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/simplenet_model.png?random=323527528432525.24234\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to \"compile\" the models. This means chosing a Loss function and an Optimizer (the algorithm that finds the minimum loss possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train our network on some data. In this case, we start by generating random data with numpy.\n",
    "\n",
    "We generate 1000 data samples made of 3 floating point inputs. We assign then to each data sample a binary label (0 or 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((10000, 3))\n",
    "labels = np.random.randint(2, size=(10000, 1))\n",
    "\n",
    "#let's print the first sample (three floats) and its corresponding label:\n",
    "#let's print some data and the corresponding label to check that they match the table above\n",
    "for x in range(5):\n",
    "    print(f\"sample [{data[x,0]: .2f}, {data[x,1]: .2f}, {data[x,2]: .2f}] has label {labels[x,0]:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally launch the training of our model on this random dataset. We submit 32 samples at a time to the network, calculate the loss on those, then the backpropagation algorithm will perform the weight update before the next 32 samples are submitted. We do this for all 1000 samples to complete the first epoch. Then we start again from the first 32 samples, and so on for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes well we'll see that... the network doesn't learn anything (50% accuracy on a binary classification problem is akin to using a coin toss to predict the outcome)! This is not unexpected, since we are training on random inputs and random labels.\n",
    "\n",
    "\n",
    "# Use case #1: modelling the XOR function \n",
    "\n",
    "But what if we wanted to train on something that makes more sense? Let's try and train this same network on the infamous XOR problem. This time, we have three inputs:\n",
    "\n",
    "<img src=\"figures/3-IP-TRUTH-TABLE2.jpg\">\n",
    "\n",
    "Let's also make another change, so that the \"0\" inputs are actually negative numbers (0 included), while the \"1\" inputs are positive numbers (0 excluded).\n",
    "\n",
    "So once more we can generate 1000 random samples of 3-float vectors with numpy, but this time we make sure to label them properly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR data\n",
    "import numpy as np\n",
    "data = np.random.random((10000, 3)) - 0.5\n",
    "labels = np.zeros((10000, 1))\n",
    "\n",
    "labels[np.where(np.logical_xor(np.logical_xor(data[:,0] > 0, data[:,1] > 0), data[:,2] > 0))] = 1\n",
    "\n",
    "#let's print some data and the corresponding label to check that they match the table above\n",
    "for x in range(5):\n",
    "    print(f\"{data[x,0]: .2f} xor {data[x,1]: .2f} xor {data[x,2]: .2f} equals {labels[x,0]:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try and visualize the data with PCA. But is it linearly separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed = pca.fit_transform(data)\n",
    "plt.scatter(transformed[:,0], transformed[:,1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train another neural network with the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "\n",
    "#We want a \"clean\" model, not the one trained on random data from before\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=3, activation='relu', input_dim=3))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "hist = model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not great, but better than random (accuracy ~60%) anyway.\n",
    "\n",
    "Now, before we try and make our XOR model better, we should address the issue of training and testing.\n",
    "\n",
    "If we do things properly, we need to have three things:\n",
    "\n",
    "* Training set\n",
    "* Validation set\n",
    "* Test set\n",
    "\n",
    "## Training set\n",
    "* Use it to fit the model's weights during the gradient descent procedure\n",
    "* It can be show many times to the model\n",
    "* Iterate until ~convergence for that model is reached\n",
    "\n",
    "## Validation set\n",
    "* The validation set is an independent set of samples that are **not in the training set** \n",
    "* Used to assess a model's performance after training\n",
    "* Used to tune the network architecture and other parameters (hyperparameters) \n",
    "* Iterate until we have found a good architecture and hyperparameters for the prediction task\n",
    "\n",
    "## Test set\n",
    "* It is independent both from training and validation set: no samples in the test set are **either in training or validation set**\n",
    "* Test samples shouldn't even be too similar to other examples in the other sets\n",
    "* Used only when the best model on the validation set has been selected \n",
    "* It tells us how well our model is able to generalise to new, previously unseen data\n",
    "\n",
    "Now, let's generate training and test set. Then, we will use a part of the training set as validation set at training time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR data\n",
    "import numpy as np\n",
    "\n",
    "#training set: 10.000 samples\n",
    "train_x = np.random.random((10000, 3))*2 - 1.0\n",
    "train_y = np.zeros((10000, 1))\n",
    "train_y[np.where(np.logical_xor(np.logical_xor(train_x[:,0] > 0, train_x[:,1] > 0), train_x[:,2] > 0))] = 1\n",
    "\n",
    "#test set: 2.000 samples\n",
    "test_x = np.random.random((2000, 3))*2 - 1.0\n",
    "test_y = np.zeros((2000, 1))\n",
    "test_y[np.where(np.logical_xor(np.logical_xor(test_x[:,0] > 0, test_x[:,1] > 0), test_x[:,2] > 0))] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set aside the test set for a moment and let's try to train and validate the previous model. We will use the \"validation_split\" parameter of the fit() function (see here for more info: https://keras.io/models/sequential/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#We want a \"clean\" model, not the one trained on random data from before\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=3, activation='tanh', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "# !!! NOTICE THE validation_split PARAMETER !!! This means that the last 10% of the training set will not\n",
    "# be used for training, just to evaluate the model.\n",
    "# This is not always a good strategy, but it works fine in this case\n",
    "hist = model.fit(train_x, train_y, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at our training on a nice plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train acc', 'val acc', 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong with the model and why does it not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #1: can you make a Keras model that beats the previous one at modelling the XOR?\n",
    "\n",
    "Take 15 minutes or so to:\n",
    "\n",
    "* experiment with Dense layers (check out https://keras.io/layers/core/ for more information)\n",
    "* activations? (\"linear\", \"sigmoid\", \"tanh\", \"softmax\", \"relu\". Others: https://keras.io/api/layers/activations/)\n",
    "* maybe try with larger datasets?\n",
    "* epoch numbers?\n",
    "* batch size?\n",
    "* different optimizers? (https://keras.io/api/optimizers/)\n",
    "* different loss functions? (https://keras.io/api/losses/)\n",
    "* it's up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#We want a \"clean\" model, not the one trained on random data from before\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=12, activation='tanh', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.compile(optimizer='adam',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "# !!! NOTICE THE validation_split PARAMETER !!! This means that the last 10% of the training set will not\n",
    "# be used for training, just to evaluate the model.\n",
    "# This is not always a good strategy, but it works fine in this case\n",
    "hist = model.fit(train_x, train_y, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have found one or two models with best validation loss/accuracy, you can check their ability to generalize to new outputs on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "(loss, accuracy) = model.evaluate(test_x, test_y)\n",
    "\n",
    "plot_loss_acc(hist)\n",
    "\n",
    "print(\"Test loss: {}, test accuracy {}\".format(loss,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case #2: classifying text data (tweets)\n",
    "\n",
    "Now, let's try something completely different. We will load a new dataset from disk, clean it up and prepare it for training. The data here is of a completely different type, as this is a set of 3000 tweets. So, we have to deal with short text inputs.\n",
    "\n",
    "Each tweet has been written by one of two well-known individuals from the world of US politics. Our task is simply to decide who wrote it. Donald or Hillary?\n",
    "\n",
    "<img src=\"figures/trump-clinton-split.jpg\">\n",
    "\n",
    "The first question here is: how do we deal with string inputs? We can't multiply a word by a weight, so we need to translate the text input in numbers before we proceed and feed it to our first layer. In this case, there are usually two options. The first is called \"one-hot\" encoding, where each word in a dictionary is translated to a vectors of ones and zeros. If there are 10 words in our dictionary (for example, the words are \"zero\", \"one\", \"two\" ... \"nine\"), each vector will contain ten elements, with nine elements set to zero and one set to one:\n",
    "\n",
    "* \"zero\" -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "* \"one\"  -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "* \"two\"  -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "* ...\n",
    "* \"nine\" -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "This is usually ok when dealing with text (or, more generically, \"categorical\") variables taken from a relatively short dictionary. In the case of tweets, we might be dealing with very a dictionary containing tens of thousands of different terms, so we would have huge inputs of sparse vectors of zeros and ones. This is not ideal.\n",
    "\n",
    "The second option is to use word embeddings, which translate each word to a vector of floating points with some nice properties, as we can see in the following image:\n",
    "\n",
    "<img src=\"figures/Word-Vectors.png\">\n",
    "\n",
    "Check out [this cool page](https://anvaka.github.io/pm/#/galaxy/word2vec-wiki?cx=-17&cy=-237&cz=-613&lx=-0.0575&ly=-0.9661&lz=-0.2401&lw=-0.0756&ml=300&s=1.75&l=1&v=d50_clean) visualizing embeddings calculated on the whole English dictionary for more examples.\n",
    "\n",
    "Embeddings are done with a special NN layer that in Keras is called simply \"Embedding\" (https://keras.io/layers/embeddings/). The Embedding layer is provided with a number of text inputs (in our case, tweets) and learns to map similar words into n-dimensional vectors that are close together in the corresponding n-dimensional space.\n",
    "\n",
    "In the following piece of code, we will start by loading the dataset with pandas (https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) and preparing it for training.\n",
    "\n",
    "Notice how before we can use the Embedding layer, we want to map each word to an integer. This is because the input to an Embedding layer is actually a set of integers, where each integer represents a word. The important thing here, is that a given word has to be mapped always to the same integer throughout the whole dataset, so that the Embedding layer can recognise it from different tweets.\n",
    "\n",
    "In this case, for example, the word \"the\" will always be mapped to the number 1, the word \"question\" to the number 2, etc.\n",
    "\n",
    "* \"The question is what to do\"      -> [1, 2, 32, 55, 87]\n",
    "* \"I don't understand the question\" -> [12, 4, 123, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweet_dataset = pd.read_csv(\"./tweets.csv\")\n",
    "\n",
    "#First of all, let's get rid of all retweets\n",
    "tweet_dataset = tweet_dataset[tweet_dataset[\"is_retweet\"] == False]\n",
    "\n",
    "#Next, let's remove all URLs, since they should not be of any help (unless we actually checked what they link to)\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "#remove \"special\" words such as twitter handles\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('@[a-zA-Z0-9]*', '', case=False)\n",
    "#remove hashtags\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('#[a-zA-Z0-9]*', '', case=False)\n",
    "\n",
    "#Now let's make sure that non-alphanumeric characters are taken as single words\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('\\s*([^a-zA-Z0-9 ])\\s*', ' \\\\1 ', case=False)\n",
    "#tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.replace('\\s*([^a-zA-Z0-9 ])\\s*', '', case=False)\n",
    "\n",
    "#make all words lower case?\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.lower()\n",
    "\n",
    "#split the tweets in list of words\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.strip()\n",
    "tweet_dataset[\"text\"] = tweet_dataset[\"text\"].str.split(\" \")\n",
    "# \"My name is hillary\" -> [\"my\", \"name\", \"is\", \"hillary\"] -> [0, 14, 5, 32, 0, 0, 0, 0, 0,...0]\n",
    "\n",
    "#since the neural networks don't really like string inputs,\n",
    "#we have to convert each word to a unique integer.\n",
    "integer_dict = {}\n",
    "integer_dict[\"padding\"] = 0\n",
    "#hillary -> 0\n",
    "#i -> 54\n",
    "#am -> 108\n",
    "#\"I am Hillary\" -> [54, 108, 0] -> [[1.0, 0.2, 0.1], [0.2...], [0.99,...]]\n",
    "word_dict = {}\n",
    "word_dict[0] = \"padding\"\n",
    "\n",
    "#assign a unique integer to each unique word\n",
    "count = 1\n",
    "for index, row in tweet_dataset.iterrows():\n",
    "    for element in row[\"text\"]:\n",
    "        if element not in integer_dict.keys():\n",
    "            \n",
    "            integer_dict[element] = count\n",
    "            word_dict[count] = element\n",
    "            count += 1\n",
    "    \n",
    "tweet_dataset[\"numbers\"] = tweet_dataset[\"text\"].apply(lambda x:[integer_dict[y] for y in x])\n",
    "\n",
    "#Let's also assign integer labels \n",
    "tweet_dataset.loc[tweet_dataset[\"handle\"] == \"realDonaldTrump\",\"label\"] = 1\n",
    "tweet_dataset.loc[tweet_dataset[\"handle\"] == \"HillaryClinton\",\"label\"] = 0\n",
    "\n",
    "#The longest tweet has 58 words, this will add padding to shorter tweets\n",
    "train_x = pd.DataFrame(tweet_dataset[\"numbers\"].values.tolist()).values\n",
    "train_x[np.where(np.isnan(train_x[:]))] = 0\n",
    "\n",
    "train_y = np.array(tweet_dataset[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how we can use Embeddings to transform our dictionary of words into a dictionary of float vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=count, output_dim=32, input_length=train_x.shape[1], name='embeddings'))\n",
    "model.add(Flatten()) #Dense layers only accept 1D inputs, so we need to flatten the output from Embedding, which is 2D\n",
    "model.add(Dense(10, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification problem of this type, the *order* of the inputs is very important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Hillary sucks, trump rules\"\n",
    "\"Trump sucks, hillary rules\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of layer is perfect for this kind of data where we are dealing with a *series* of ordered inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(count, 32, input_length=train_x.shape[1], name='embeddings'))\n",
    "model.add(LSTM(5, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(2, return_sequences=False, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "#model.summary()\n",
    "hist = model.fit(train_x, train_y, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the outputs of the embedding layer. We extract the embedding layer from the trained model and we use it to calculate embeddings for every word in our dictionary. Then, we map the 32-dimensional output vector onto 2 dimensions with the help of principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.pyplot import scatter\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('embeddings').output)\n",
    "\n",
    "n_words = 1000\n",
    "inputs = np.zeros((n_words, train_x.shape[1]))\n",
    "for i in range(n_words):\n",
    "    word = word_dict[i]\n",
    "    for count in range(train_x.shape[1]):\n",
    "        inputs[i, count] = i\n",
    "    \n",
    "intermediate_output = intermediate_layer_model.predict(inputs)\n",
    "\n",
    "points = np.squeeze(intermediate_output[:,0,:])\n",
    "\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "points_pca = pca.fit_transform(points)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(points_pca[:,0], points_pca[:,1], 'o')\n",
    "\n",
    "for i in range(n_words):\n",
    "    ax.annotate(word_dict[i], (points_pca[i,0], points_pca[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2, what is wrong (or right) with this dataset?\n",
    "\n",
    "* Take a few minutes to analyze the word cloud. Can you see what kind of words make the classification easier? If you wanted to make a less biased (and harder to classify) dataset, what would you change? \n",
    "\n",
    "* If possible, go back to the dataset generation step and remove words that make the classification task too easy. Then, try and train a new network. Are the results the same as before? (Learn a bit more about data leakage: https://machinelearningmastery.com/data-leakage-machine-learning/)\n",
    "\n",
    "* Now that we have trained and validated our model, what would you suggest using as test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use case #3: classifying images with convolutional networks\n",
    "\n",
    "The third and final use case is an example of use of the (in)famous convolutional networks. We will try and recognise objects from pictures, which means (once more) classifying images in a set of predefined classes (is it a bird? Is it a plane? <s>Is it Superman?</s>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# example of loading the cifar10 dataset\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "# load dataset\n",
    "pyplot.subplots_adjust(hspace=1)\n",
    "classes = {0 : \"airplane\", 1 : \"automobile\", 2 : \"bird\", 3 : \"cat\", 4 : \"deer\", 5 : \"dog\", 6 : \"frog\", 7 : \"horse\", 8 : \"ship\", 9 : \"truck\"}\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (train_x.shape, train_y.shape))\n",
    "print('Test: X=%s, y=%s' % (test_x.shape, test_y.shape))\n",
    "# plot first few images\n",
    "for i in range(16):\n",
    "    # define subplot\n",
    "    ax = pyplot.subplot(4,4, 1 + i)\n",
    "    ax.set_xlabel(classes[train_y[i,0]])\n",
    "    # plot raw pixel data\n",
    "    pyplot.imshow(train_x[i])\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to classify each image in the correct category. Let's whip out Keras again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, (5, 5), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, (9, 9), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (11, 11)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test),\n",
    "            shuffle=True)\n",
    "\n",
    "#If you want, you can save a model to disk by uncommenting the next few lines\n",
    "#if not os.path.isdir(save_dir):\n",
    "#    os.makedirs(save_dir)\n",
    "#model_path = os.path.join(save_dir, model_name)\n",
    "#model.save(model_path)\n",
    "#print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[4].get_weights()\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "    # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras also allows you to load predefined models to train on this or similar datasets. A list of these models [is available here](https://keras.io/api/applications/). If we choose, for example MobileNet, a network known for getting good results with relatively little computational power, we can load it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = keras.applications.MobileNetV2(\n",
    "    input_shape=(32, 32, 3,),\n",
    "    alpha=1.0,\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=10,\n",
    "    classifier_activation=\"softmax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mobilenet.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test),\n",
    "            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3, can you build a CNN according to these specs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you become a Neural Network architect, it is probably a good idea to be able to build from a predefined blueprint. Given the figure below, it should be possible to build a CNN model for the prediction of Cifar-10 classes. \n",
    "\n",
    "* Write your own code to match the specs in the figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Neural_Network_Model_Definition.jpg-1040x0.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is missing from the figure to fully determine the architecture? \n",
    "* What kind of design choices can we make to fill in the gaps?\n",
    "* Do you think that this network would work better than our first example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(...)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build fancier nets with Keras' functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our very first model was built using the Sequential function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=3, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes it is not possible to use sequential models. For example if we wanted to have a more complex model with multiple inputs and/or multiple outputs. This is where the functional API comes to the rescue! It is pretty simple to use once we can appreciate the difference between a layer and a tensor. Here is the exact same network, written with the functional API instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "#declare the layers first\n",
    "this_input = Input(shape=(3,))\n",
    "dense1 = Dense(3, activation='relu')\n",
    "dense2 = Dense(2, activation='softmax')\n",
    "\n",
    "#then use the layers as functions\n",
    "dense1_out = dense1(this_input)\n",
    "dense2_out = dense2(dense1_out)\n",
    "model = Model(this_input, dense2_out)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we write a network with two separate input layers and two separate output layers? Here is an example from the Keras docs (https://keras.io/guides/functional_api/):\n",
    "\n",
    ">For example, if you're building a system for ranking custom issue tickets by priority and routing them to the correct department, then the model will have three inputs:\n",
    "\n",
    ">* the title of the ticket (text input),\n",
    ">* the text body of the ticket (text input), and\n",
    ">* any tags added by the user (categorical input)\n",
    "\n",
    ">This model will have two outputs:\n",
    "\n",
    ">* the priority score between 0 and 1 (scalar sigmoid output), and\n",
    ">* the department that should handle the ticket (softmax output over the set of departments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, concatenate\n",
    "from keras.utils import plot_model\n",
    "\n",
    "num_tags = 12  # Number of unique issue tags\n",
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "title_input = Input(\n",
    "    shape=(None,), name=\"title\"\n",
    ")  # Variable-length sequence of ints\n",
    "body_input = Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "tags_input = Input(\n",
    "    shape=(num_tags,), name=\"tags\"\n",
    ")  # Binary vectors of size `num_tags`\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = Embedding(num_words, 64)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = Embedding(num_words, 64)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = LSTM(128)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = LSTM(32)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = concatenate([title_features, body_features, tags_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = Dense(1, name=\"priority\")(x)\n",
    "# Stick a department classifier on top of the features\n",
    "department_pred = Dense(num_departments, name=\"department\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = Model(\n",
    "    inputs=[title_input, body_input, tags_input],\n",
    "    outputs=[priority_pred, department_pred],\n",
    ")\n",
    "\n",
    "plot_model(model, \"figures/multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/multi_input_and_output_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4, can you re-build the previous CNN using the functional model?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
